## 分布式
## 分布式协议与算法实战 - 韩健
https://time.geekbang.org/column/intro/100046101?tab=intro

### 开篇词 | 想成为分布式高手？那就先把协议和算法烂熟于心吧

### 01 | 拜占庭将军问题：有叛徒的情况下，如何才能达成共识？
口信消息型拜占庭问题：将军人数≥3m+1。
签名消息型拜占庭问题：验证签名解决二忠一叛。
BFT拜占庭容错算法：BFT/PBFT/PoW
CFT故障容错算法：Paxos/Raft/ZAB

### 02 | CAP理论：分布式系统的PH试纸，用它来测酸碱度
 CA： 非分布式系统。比如单机版MySQL
 CP: ZK/Etcd/HBase
 AP: Cassandra/DynamoDB
通过延迟衡量服务可用性。

### 03 | ACID理论：CAP的酸，追求一致性
XA协议是X/Open国际联盟基于二阶段提交协议提出的，也叫X/Open Distributed Transaction Processing(DTP)模型，比如MySQL XA实现分布式事务。 存在问题： 锁定资源影响性能。
TCC。本质是补偿事务。
ACID理解为CAP中最强的一致性。

### 04 | BASE理论：CAP的碱，追求可用性
BA：流量削峰、体验降级、延迟响应、过载保护。
SE: 自定义 写一致级别：All，Quorum，One，Any。实现方式：异步修复、写时修复、读时修复。
BASE在NoSQL中应用广泛。

### 05 | Paxos算法（一）：如何在多个节点间确定某变量的值？

### 07丨Raft算法（一）：如何选举领导者
Raft 算法实现了随机超时时间的特性。也就是说，每个节点等待领导者节点心跳信息的超时时间间隔是随机的。通过上面的图片你可以看到，集群中没有领导者，而节点 A 的等待超时时间最小（150ms），它会最先因为没有等到领导者的心跳信息，发生超时。
节点 A 当选领导者后，他将周期性地发送心跳消息，通知其他服务器我是领导者，阻止跟随者发起新的选举，篡权。
两个随机时间： 1. 正常运行中，跟随者等待领导者心跳信息超时的时间间隔，是随机的；2. 选举过程中，等待选举超时的时间间隔。
投票时比较日志索引大小使用的是已复制未提交的uncommitted类日志

### 08丨Raft算法（二）：如何复制日志？
在 Raft 算法中，副本数据是以日志的形式存在的，领导者接收到来自客户端写请求后，处理写请求的过程就是一个复制和提交日志项的过程。
你可以把 Raft 的日志复制理解成一个优化后的二阶段提交（将二阶段优化成了一阶段），减少了一半的往返消息，也就是降低了一半的消息延迟。那日志复制的具体过程是什么呢？
这是 Raft 中的一个优化，领导者不直接发送消息通知其他节点提交指定日志项。因为领导者的日志复制 RPC 消息或心跳消息，包含了当前最大的，将会被提交的日志项索引值。所以通过日志复制 RPC 消息或心跳消息，跟随者就可以知道领导者的日志提交位置信息。
领导者通过日志复制 RPC 一致性检查，找到跟随者节点上与自己相同日志项的最大索引值，然后复制并更新覆盖该索引值之后的日志项，实现了各节点日志的一致。需要你注意的是，跟随者中的不一致日志项会被领导者的日志覆盖，而且领导者从来不会覆盖或者删除自己的日志。
leader对每个跟随者，单独维护和自己日志项相同的最大值。
存在问题： 如果所有的read都从leader上取数据，follow节点的性能就浪费了。
若跟随者和leader差的远，多次RPC调用才能开始同步，优化：hashicorp raft会采用pipeline模式进行日志复制。

### 09 | Raft算法（三）：如何解决成员变更的问题？
成员变更：单节点变更，一次只变更一个节点。如Hashicorp Raft、Etcd
Raft不是一致性算法而是共识算法，是Multi-Paxos算法。
Consul三种一致性模型：default（读未验证的leader）、consistent（读已验证的leader）、stale（任意节点）。
最初实现成员变更的是联合共识（Joint Consensus），但这个方法实现起来难，后来 Raft 的作者就提出了一种改进后的方法，单节点变更（single-server changes）。
在集群中进行成员变更的最大风险是，可能会同时出现 2 个领导者。比如新增2个节点DE：节点 A（leader）、B 和 C 之间发生了分区错误，节点 A、B 组成旧配置中的“大多数”（配置N=3）； 而C、D、E组成了大多数（配置N=5）。 
在分区错误、节点故障等情况下，如果我们并发执行单节点变更，那么就可能出现一次单节点变更尚未完成，新的单节点变更又在执行，导致集群出现 2 个领导者的情况。
绝大多数 Raft 采用的都是单节点变更的方法（比如 Etcd、Hashicorp Raft）。其中，Hashicorp Raft 单节点变更的实现，是由 Raft 算法的作者迭戈·安加罗（Diego Ongaro）设计的，很有参考价值。
在实际工程中，Consul 的 consistent 就够用了，可以不用线性一致性。
比如我负责过多个 QQ 后台的海量服务分布式系统，其中配置中心、名字服务以及时序数据库的 META 节点，采用了 Raft 算法。在设计时序数据库的 DATA 节点一致性时，基于水平扩展、性能和数据完整性等考虑，就没采用 Raft 算法，而是采用了 Quorum NWR、失败重传、反熵等机制。这样安排不仅满足了业务的需求，还通过尽可能采用最终一致性方案的方式，实现系统的高性能，降低了成本。

### 10 | 一致哈希算法：如何分群，突破集群的“领导者”限制？
为了便于你理解，我举个例子，对于 1000 万 key 的 3 节点 KV 存储，如果我们增加 1 个节点，变为 4 节点集群，则需要迁移 75% 的数据。
让我们一起来看一个例子。使用一致哈希的话，对于 1000 万 key 的 3 节点 KV 存储，如果我们增加 1 个节点，变为 4 节点集群，只需要迁移 24.3% 的数据：
3 个节点到4 个节点， hash迁移75%，     consistent-hash 迁移24.3%
10个节点到11个节点， hash迁移90.909%， consistent-hash 迁移6.479%

### 11 | Gossip协议：流言蜚语，原来也可以实现一致性
Gossip 的三板斧分别是：直接邮寄（Direct Mail）、反熵（Anti-entropy）和谣言传播（Rumor mongering）。
反熵中的熵是指混乱程度，反熵就是指消除不同节点中数据的差异，提升节点间数据的相似度，降低熵值。
因为反熵需要节点两两交换和比对自己所有的数据，执行反熵时通讯成本会很高，所以我不建议你在实际场景中频繁执行反熵，并且可以通过引入校验和（Checksum）等机制，降低需要对比的数据量和通讯消息等。
虽然反熵很实用，但是执行反熵时，相关的节点都是已知的，而且节点数量不能太多，如果是一个动态变化或节点数比较多的分布式环境（比如在 DevOps 环境中检测节点故障，并动态维护集群节点状态），这时反熵就不适用了。那么当你面临这个情况要怎样实现最终一致性呢？答案就是谣言传播。
谣言传播非常具有传染性，它适合动态变化的分布式系统。
不同节点分片数据不一致。按照一定顺序来修复节点的数据差异，先随机选择一个节点，然后循环修复，每个节点生成自己节点有、下一个节点没有的差异数据，发送给下一个节点，进行修复（为了方便演示，假设 Shard1、Shard2 在各节点上是不一致的）：
反熵需要做一致性对比，很消耗系统性能，所以建议你将是否启用反熵功能、执行一致性检测的时间间隔等，做成可配置的，能在不同场景中按需使用。
作为一种异步修复、实现最终一致性的协议，反熵在存储组件中应用广泛，比如 Dynamo、InfluxDB、Cassandra，我希望你能彻底掌握反熵的实现方法，在后续工作中，需要实现最终一致性时，优先考虑反熵。
因为谣言传播具有传染性，一个节点传给了另一个节点，另一个节点又将充当传播者，传染给其他节点，所以非常适合动态变化的分布式系统，比如 Cassandra 采用这种方式动态管理集群节点状态。
在实际场景中，实现数据副本的最终一致性时，一般而言，直接邮寄的方式是一定要实现的，因为不需要做一致性对比，只是通过发送更新数据或缓存重传，来修复数据的不一致，性能损耗低。在存储组件中，节点都是已知的，一般采用反熵修复数据副本的一致性。当集群节点是变化的，或者集群节点数比较多时，这时要采用谣言传播的方式，同步更新数据，实现最终一致。

### 12 | Quorum NWR算法：想要灵活地自定义一致性，没问题！
其实，在 AP 型分布式系统中（比如 Dynamo、Cassandra、InfluxDB 企业版的 DATA 节点集群），Quorum NWR 是通常都会实现的一个功能，很常用。
副本数可以不等于节点数，不同的数据可以有不同的副本数。
W，又称写一致性级别（Write Consistency Level），表示成功完成 W 个副本更新，才完成写操作
R，又称读一致性级别（Read Consistency Level），表示读取一个数据对象时需要读 R 个副本。你可以这么理解，读取指定数据时，要读 R 副本，然后返回 R 个副本中最新的那份数据
当 W + R > N 的时候，对于客户端来讲，整个系统能保证强一致性，一定能返回更新后的那份数据。
influxDB， create retention policy “rp_one_day” on “telegraf” duration 1d replication 3
InfluxDB写一致性级别 : any：(包括Hinted-handoff 缓存) 、one、quorum、all 
当 W + R > N 时，可以实现强一致性。另外，如何设置 N、W、R 值，取决于我们想优化哪方面的性能。比如，N 决定了副本的冗余备份能力；如果设置 W = N，读性能比较好；如果设置 R = N，写性能比较好；如果设置 W = (N + 1) / 2、R = (N + 1) / 2，容错能力比较好，能容忍少数节点（也就是 (N - 1) / 2）的故障。
Quorum NWR 是非常实用的一个算法，能有效弥补 AP 型系统缺乏强一致性的痛点，给业务提供了按需选择一致性级别的灵活度，建议你的开发实现 AP 型系统时，也实现 Quorum NWR。

### 13 | PBFT算法：有人作恶，如何达成共识？

### 14 | PoW算法：有办法黑比特币吗？

### 15 | ZAB协议：如何实现操作的顺序性？

### 16 | InfluxDB企业版一致性实现剖析：他山之石，可以攻玉
通过自定义副本数、Hinted-handoff、反熵、Quorum NWR 等技术，我们能实现 AP 型分布式系统，还能通过水平扩展，高效扩展集群的读写能力。
META 节点存放的是系统运行的关键元信息，比如数据库（Database）、表（Measurement）、保留策略（Retention policy）等。它的特点是一致性敏感，但读写访问量不高，需要一定的容错能力。
DATA 节点存放的是具体的时序数据。它有这样几个特点：最终一致性、面向业务、性能越高越好，除了容错，还需要实现水平扩展，扩展集群的读写性能。
基于不同场景特点的考虑，2 个单独程序更合适。如果 META 节点和 DATA 节点合并为一个程序，因读写性能需要，设计了一个 10 节点的 DATA 节点集群，这就意味着 META 节点集群（Raft 集群）也是 10 个节点。在学了 Raft 算法之后，你应该知道，这时就会出现消息数多、日志提交慢的问题，肯定不行了。
InfluxDB Hinted-handoff 实现: 写失败的请求，会缓存到本地硬盘上 ;周期性地尝试重传 ;相关参数信息，比如缓存空间大小 (max-szie)、缓存周期（max-age）、尝试间隔（retry-interval）等，是可配置的。
虽然 Hinted-handoff 可以通过重传的方式来处理数据不一致的问题，但当写失败请求的数据大于本地缓存空间时，比如某个节点长期故障，写请求的数据还是会丢失的，最终的节点的数据还是不一致的，那么怎么实现数据的最终一致性呢？答案是反熵。
数据副本之间的数据不一致，是因为数据写失败导致数据丢失了，也就是说，存在的都是合理的，缺失的就是需要修复的。这时我们可以采用两两对比、添加缺失数据的方式，来修复各数据副本的不一致了。
通过 Raft 算法，我们能实现强一致性的分布式系统，能保证写操作完成后，后续所有的读操作，都能读取到最新的数据。
相比 OpenTSDB，InfluxDB 的写性能是它的 9.96 倍，存储效率是它的 8.69 倍，查询效率是它的 7.38 倍。
相比 Graphite，InfluxDB 的写性能是它的 12 倍，存储效率是 6.3 倍，查询效率是 9 倍。
最后我想说的是，我反对堆砌开源软件，建议谨慎引入 Kafka 等缓存中间件。老话说，在计算机中，任何问题都可以通过引入一个中间层来解决。这句话是正确的，但背后的成本是不容忽视的，尤其是在海量系统中。我的建议是直面问题，通过技术手段在代码和架构层面解决它，而不是引入和堆砌更多的开源软件。

### 17 | Hashicorp Raft（一）：如何跨过理论和代码之间的鸿沟？
在不需要进行日志一致性检测，复制功能已正常运行的时候，开启了流水线复制模式，
Hashicorp Raft 实现，是常用的 Golang 版 Raft 算法的实现，被众多流行软件使用，如 Consul、InfluxDB、IPFS 等. 
其他实现： Go-Raft、LogCabin、Willemt-Raft

### 18 | Hashicorp Raft（二）：如何以“集群节点”为中心使用API？
NewRaft(Config, FSM, LogStore, StableStore, SnapshotStore, Transport)
FSM有限状态机： 日志处理的核心实现。
LogStore 存储的是 Raft 日志，你可以用 raft-boltdb 来实现底层存储，持久化存储数据。在这里我想说的是，raft-boltdb 是 Hashicorp 团队专门为 Hashicorp Raft 持久化存储，而开发设计的，使用广泛，打磨充分。 
StableStore 使用 raft-boltdb 存储节点关键状态信息，如任期编号
SnapshotStore建议使用 FileSnapshotStore 实现快照， 使用文件持久化存储，避免因程序重启，导致快照数据丢失。
Transport 建议 TCPTransport.
除了提到的 raft-boltdb 做作为 LogStore 和 StableStore，也可以调用 NewInmemStore() 创建内存型存储，在测试时比较方便，重新执行程序进行测试时，不需要手动清理数据存储。
你可以通过 Raft.Stats() 函数，查看集群的内部统计信息，比如节点状态、任期编号、节点数等，这在调试或确认节点运行状况的时候很有用。

### 19 | 基于Raft的分布式KV系统开发实战（一）：如何设计架构？
kv幂等性。
写操作两个方案， 1是follower返回leader信息，client重发信息。 2是follower转发信息。 
follower转发方案会引入一个中间节点（跟随者），增加了问题分析排查的复杂度。而且，一般情况下，在绝大部分的时间内（比如 Google Chubby 团队观察到的值是数天），领导者是处于稳定状态的，某个节点一直是领导者，那么引入中间节点，就会增加大量的不必要的消息和性能消耗。所以，综合考虑，推荐client实现方法。
读操作，关乎一致性的实现，推荐实现 default、consistent、stale 三种一致性模型，将一致性的选择权交给用户，让用户根据实际业务特点，按需选择，灵活使用。
在实际系统中，你可以统计热数据的命中率，并根据命中率来动态调整冷热模型。在这里，我想强调的是，冷热分离理念在设计海量数据存储系统时尤为重要，比如，自研 KV 存储的成本仅为 Redis 数十分之一，其中系统设计时非常重要的一个理念就是冷热分离。希望你能重视这个理念，在实际场景中活学活用。

### 20 | 基于Raft的分布式KV系统开发实战（二）：如何实现代码？


